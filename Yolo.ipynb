{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNVU7eu9CQj3"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this project, I am going to train the tiny version of [YOLOv4](https://arxiv.org/pdf/2004.10934.pdf) on my own dataset, [YOLOv4 tiny](https://github.com/AlexeyAB/darknet/issues/6067).\n",
        "\n",
        "Dataset consists of screenshots of blackjack desks found on the Internet as well as some taken by myself. Screenshots were altered in order to increase the size of dataset using Roboflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDTvGt2zt7cm"
      },
      "source": [
        "# Configuring cuDNN on Colab for YOLOv4\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Yolov4 from Darknet to work I'm going to need cuDNN. I'm following Darknet creator's guide for this step."
      ],
      "metadata": {
        "id": "4JPsNcdo67KV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-bguKWgtxSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de443422-a17b-4c11-97b7-9f49505029eb"
      },
      "source": [
        "# CUDA: Let's check that Nvidia CUDA drivers are already pre-installed and which version is it.\n",
        "!/usr/local/cuda/bin/nvcc --version\n",
        "# We need to install the correct cuDNN according to this output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6BRAVo182G5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b81bd04-5c6f-4c9a-f1d2-e68a9ea12561"
      },
      "source": [
        "#take a look at the kind of GPU we have\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 16 11:58:02 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    13W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb5mFF-RyBAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870f2776-f04c-499a-bdca-6e5587c1367b"
      },
      "source": [
        "# This cell ensures you have the correct architecture for your respective GPU\n",
        "# If you command is not found, look through these GPUs, find the respective\n",
        "# GPU and add them to the archTypes dictionary\n",
        "\n",
        "# Tesla V100\n",
        "# ARCH= -gencode arch=compute_70,code=[sm_70,compute_70]\n",
        "\n",
        "# Tesla K80 \n",
        "# ARCH= -gencode arch=compute_37,code=sm_37\n",
        "\n",
        "# GeForce RTX 2080 Ti, RTX 2080, RTX 2070, Quadro RTX 8000, Quadro RTX 6000, Quadro RTX 5000, Tesla T4, XNOR Tensor Cores\n",
        "# ARCH= -gencode arch=compute_75,code=[sm_75,compute_75]\n",
        "\n",
        "# Jetson XAVIER\n",
        "# ARCH= -gencode arch=compute_72,code=[sm_72,compute_72]\n",
        "\n",
        "# GTX 1080, GTX 1070, GTX 1060, GTX 1050, GTX 1030, Titan Xp, Tesla P40, Tesla P4\n",
        "# ARCH= -gencode arch=compute_61,code=sm_61\n",
        "\n",
        "# GP100/Tesla P100 - DGX-1\n",
        "# ARCH= -gencode arch=compute_60,code=sm_60\n",
        "\n",
        "# For Jetson TX1, Tegra X1, DRIVE CX, DRIVE PX - uncomment:\n",
        "# ARCH= -gencode arch=compute_53,code=[sm_53,compute_53]\n",
        "\n",
        "# For Jetson Tx2 or Drive-PX2 uncomment:\n",
        "# ARCH= -gencode arch=compute_62,code=[sm_62,compute_62]\n",
        "import os\n",
        "os.environ['GPU_TYPE'] = str(os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read())\n",
        "\n",
        "def getGPUArch(argument):\n",
        "  try:\n",
        "    argument = argument.strip()\n",
        "    # All Colab GPUs\n",
        "    archTypes = {\n",
        "        \"Tesla V100-SXM2-16GB\": \"-gencode arch=compute_70,code=[sm_70,compute_70]\",\n",
        "        \"Tesla K80\": \"-gencode arch=compute_37,code=sm_37\",\n",
        "        \"Tesla T4\": \"-gencode arch=compute_75,code=[sm_75,compute_75]\",\n",
        "        \"Tesla P40\": \"-gencode arch=compute_61,code=sm_61\",\n",
        "        \"Tesla P4\": \"-gencode arch=compute_61,code=sm_61\",\n",
        "        \"Tesla P100-PCIE-16GB\": \"-gencode arch=compute_60,code=sm_60\"\n",
        "\n",
        "      }\n",
        "    return archTypes[argument]\n",
        "  except KeyError:\n",
        "    return \"GPU must be added to GPU Commands\"\n",
        "os.environ['ARCH_VALUE'] = getGPUArch(os.environ['GPU_TYPE'])\n",
        "\n",
        "print(\"GPU Type: \" + os.environ['GPU_TYPE'])\n",
        "print(\"ARCH Value: \" + os.environ['ARCH_VALUE'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Type: Tesla T4\n",
            "\n",
            "ARCH Value: -gencode arch=compute_75,code=[sm_75,compute_75]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16pvdFMa1FEe"
      },
      "source": [
        "# Installing Darknet for YOLOv4 on Colab\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9uY-38P93oz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114cf6a0-34fb-4dcc-b8f5-d83cb9016b55"
      },
      "source": [
        "%cd /content/\n",
        "%rm -rf darknet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQEktcfj9y9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95ccc63-8e9e-4711-bce6-9c192b85d4db"
      },
      "source": [
        "#I'm cloning the fork of darknet maintained by roboflow\n",
        "#small changes have been made to configure darknet for training\n",
        "!git clone https://github.com/roboflow-ai/darknet.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'darknet'...\n",
            "remote: Enumerating objects: 13289, done.\u001b[K\n",
            "remote: Total 13289 (delta 0), reused 0 (delta 0), pack-reused 13289\u001b[K\n",
            "Receiving objects: 100% (13289/13289), 12.17 MiB | 19.05 MiB/s, done.\n",
            "Resolving deltas: 100% (9047/9047), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyMBDkaL-Aep"
      },
      "source": [
        "#installing environment from the Makefile\n",
        "%cd /content/darknet/\n",
        "# compute_37, sm_37 for Tesla K80\n",
        "# compute_75, sm_75 for Tesla T4\n",
        "# !sed -i 's/ARCH= -gencode arch=compute_60,code=sm_60/ARCH= -gencode arch=compute_75,code=sm_75/g' Makefile\n",
        "\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/g' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/g' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/g' Makefile\n",
        "!sed -i \"s/ARCH= -gencode arch=compute_60,code=sm_60/ARCH= ${ARCH_VALUE}/g\" Makefile\n",
        "!make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGPDEjfAALrQ"
      },
      "source": [
        "#downloading the newly released yolov4-tiny weights\n",
        "%cd /content/darknet\n",
        "\n",
        "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights\n",
        "!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny.cfg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWOiKj37l4wW"
      },
      "source": [
        "# Set up Custom Dataset for YOLOv4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbniFj-eSimL"
      },
      "source": [
        "As mentioned before, I'm going to be using my own dataset created using Roboflow. I'll be downloading it from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "SCQmTEbJV2sT",
        "outputId": "788e2836-9f6e-428c-deeb-d2c33102b5e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/darknet/dataset\"\n",
        "!unzip -d \"/content/darknet/dataset\" \"/content/gdrive/MyDrive/cards/annotated-non-mirrored-suit-number.zip\""
      ],
      "metadata": {
        "id": "y2eAGLqUWBkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a list of class names."
      ],
      "metadata": {
        "id": "74P5qErU8UGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/darknet/dataset/obj.names', 'w') as out:\n",
        "  st = \"\"\"10c\n",
        "10d\n",
        "10h\n",
        "10s\n",
        "2c\n",
        "2d\n",
        "2h\n",
        "2s\n",
        "3c\n",
        "3d\n",
        "3h\n",
        "3s\n",
        "4c\n",
        "4d\n",
        "4h\n",
        "4s\n",
        "5c\n",
        "5d\n",
        "5h\n",
        "5s\n",
        "6c\n",
        "6d\n",
        "6h\n",
        "6s\n",
        "7c\n",
        "7d\n",
        "7h\n",
        "7s\n",
        "8c\n",
        "8d\n",
        "8h\n",
        "8s\n",
        "9c\n",
        "9d\n",
        "9h\n",
        "9s\n",
        "Ac\n",
        "Ad\n",
        "Ah\n",
        "As\n",
        "Jc\n",
        "Jd\n",
        "Jh\n",
        "Js\n",
        "Kc\n",
        "Kd\n",
        "Kh\n",
        "Ks\n",
        "Qc\n",
        "Qd\n",
        "Qh\n",
        "Qs\"\"\"\n",
        "  out.write(st)"
      ],
      "metadata": {
        "id": "mnt_KbxrbFys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a list of paths for training algorithm."
      ],
      "metadata": {
        "id": "YORpVxjN8aSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/darknet/dataset/obj.data', 'w') as out:\n",
        "  out.write('classes = 52\\n')\n",
        "  out.write('train =/content/darknet/dataset/train.txt\\n')\n",
        "  out.write('valid = /content/darknet/dataset/valid.txt\\n')\n",
        "  out.write('names = /content/darknet/dataset/obj.names\\n')\n",
        "  out.write('backup = /content/gdrive/MyDrive/Runs/improved_dataset/\\n')"
      ],
      "metadata": {
        "id": "rYIxQkPJbhBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/darknet/dataset/train/labels/*.txt /content/darknet/dataset/train/images/\n",
        "!cp /content/darknet/dataset/valid/labels/*.txt /content/darknet/dataset/valid/images/"
      ],
      "metadata": {
        "id": "uq0DZIi9cXiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/darknet/dataset/train/images/ |wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNX0ppdecGYd",
        "outputId": "6dd5a67c-35fa-4f76-e509-d6a2e82e0fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/roboflow/\"\n",
        "!mkdir \"/content/roboflow/dataset\"\n",
        "!unzip -d \"/content/roboflow/dataset/\" \"/content/gdrive/MyDrive/cards/final_bj_ds.zip\""
      ],
      "metadata": {
        "id": "yDqoEmjuaptq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/roboflow/dataset/train/images/ |wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KM3zvCLcXFa",
        "outputId": "144a583d-33f8-4f2e-df36-1b620d1a72fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/roboflow/dataset/train/images/*.jpg /content/darknet/dataset/train/images/\n",
        "!cp /content/roboflow/dataset/valid/images/*.jpg /content/darknet/dataset/valid/images/"
      ],
      "metadata": {
        "id": "JqM4YYeCW7u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/roboflow/dataset/train/images/*.txt /content/darknet/dataset/train/images/\n",
        "!cp /content/roboflow/dataset/valid/images/*.txt /content/darknet/dataset/valid/images/"
      ],
      "metadata": {
        "id": "VP0HZPbZXffY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/darknet/dataset/train/images/ |wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqNnSNuaZDcY",
        "outputId": "ab9587d0-baa4-492f-fba6-76c3370de834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiCILEbs1NII"
      },
      "source": [
        "#writing train file (just the image list)\n",
        "import os\n",
        "\n",
        "with open('/content/darknet/dataset/train.txt', 'w') as out:\n",
        "  for img in [f for f in os.listdir('/content/darknet/dataset/train/images') if f.endswith('.jpg')]:\n",
        "    out.write('/content/darknet/dataset/train/images/' + img + '\\n')\n",
        "\n",
        "#writing the valid file (just the image list)\n",
        "import os\n",
        "\n",
        "with open('/content/darknet/dataset/valid.txt', 'w') as out:\n",
        "  for img in [f for f in os.listdir('/content/darknet/dataset/valid/images') if f.endswith('.jpg')]:\n",
        "    out.write('/content/darknet/dataset/valid/images/' + img + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HtRqO3QvjkP"
      },
      "source": [
        "# Write Custom Training Config for YOLOv4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_WJcqHhpeVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db46f1f-9519-477d-c38c-bd4bb7b412dc"
      },
      "source": [
        "#Instructions from the darknet repo\n",
        "#change line max_batches to (classes*2000 but not less than number of training images, and not less than 6000), f.e. max_batches=6000 if you train for 3 classes\n",
        "#change line steps to 80% and 90% of max_batches, f.e. steps=4800,5400\n",
        "\n",
        "def file_len(fname):\n",
        "  with open(fname) as f:\n",
        "    for i, l in enumerate(f):\n",
        "      pass\n",
        "  return i + 1\n",
        "\n",
        "num_classes = file_len('/content/darknet/dataset/obj.names')\n",
        "max_batches = num_classes*2000\n",
        "steps1 = .8 * max_batches\n",
        "steps2 = .9 * max_batches\n",
        "steps_str = str(steps1)+','+str(steps2)\n",
        "num_filters = (num_classes + 5) * 3\n",
        "\n",
        "\n",
        "print(\"writing config for a custom YOLOv4 detector detecting number of classes: \" + str(num_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "writing config for a custom YOLOv4 detector detecting number of classes: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes, max_batches, steps1, steps2, steps_str, num_filters"
      ],
      "metadata": {
        "id": "dFPYF6xKf1Xn",
        "outputId": "6f0dc92e-4c7f-4c31-9803-dff0e3a98fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52, 104000, 83200.0, 93600.0, '83200.0,93600.0', 171)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03VuD4NHnxFx"
      },
      "source": [
        "with open('/content/darknet/yolov4_custom.cfg', 'w') as out:\n",
        "  st = \"\"\"[net]\n",
        "# Testing\n",
        "#batch=1\n",
        "#subdivisions=1\n",
        "# Training\n",
        "batch=64\n",
        "subdivisions=16\n",
        "width=416\n",
        "height=416\n",
        "channels=3\n",
        "momentum=0.9\n",
        "decay=0.0005\n",
        "angle=0\n",
        "saturation = 1.5\n",
        "exposure = 1.5\n",
        "hue=.1\n",
        "\n",
        "learning_rate=0.00261\n",
        "burn_in=1000\n",
        "\n",
        "max_batches = 104000\n",
        "policy=steps\n",
        "steps=83200,93600\n",
        "scales=.1,.1\n",
        "\n",
        "\n",
        "#weights_reject_freq=1001\n",
        "#ema_alpha=0.9998\n",
        "#equidistant_point=1000\n",
        "#num_sigmas_reject_badlabels=3\n",
        "#badlabels_rejection_percentage=0.2\n",
        "\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=32\n",
        "size=3\n",
        "stride=2\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=64\n",
        "size=3\n",
        "stride=2\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=64\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers=-1\n",
        "groups=2\n",
        "group_id=1\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=32\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=32\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -1,-2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=64\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -6,-1\n",
        "\n",
        "[maxpool]\n",
        "size=2\n",
        "stride=2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=128\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers=-1\n",
        "groups=2\n",
        "group_id=1\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=64\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=64\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -1,-2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=128\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -6,-1\n",
        "\n",
        "[maxpool]\n",
        "size=2\n",
        "stride=2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=256\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers=-1\n",
        "groups=2\n",
        "group_id=1\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=128\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=128\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -1,-2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=256\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -6,-1\n",
        "\n",
        "[maxpool]\n",
        "size=2\n",
        "stride=2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=512\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "##################################\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=256\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=512\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "filters=171\n",
        "activation=linear\n",
        "\n",
        "\n",
        "\n",
        "[yolo]\n",
        "mask = 3,4,5\n",
        "anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319\n",
        "classes=52\n",
        "num=6\n",
        "jitter=.3\n",
        "scale_x_y = 1.05\n",
        "cls_normalizer=1.0\n",
        "iou_normalizer=0.07\n",
        "iou_loss=ciou\n",
        "ignore_thresh = .7\n",
        "truth_thresh = 1\n",
        "random=0\n",
        "resize=1.5\n",
        "nms_kind=greedynms\n",
        "beta_nms=0.6\n",
        "#new_coords=1\n",
        "#scale_x_y = 2.0\n",
        "\n",
        "[route]\n",
        "layers = -4\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=128\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[upsample]\n",
        "stride=2\n",
        "\n",
        "[route]\n",
        "layers = -1, 23\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=256\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "filters=171\n",
        "activation=linear\n",
        "\n",
        "[yolo]\n",
        "mask = 1,2,3\n",
        "anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319\n",
        "classes=52\n",
        "num=6\n",
        "jitter=.3\n",
        "scale_x_y = 1.05\n",
        "cls_normalizer=1.0\n",
        "iou_normalizer=0.07\n",
        "iou_loss=ciou\n",
        "ignore_thresh = .7\n",
        "truth_thresh = 1\n",
        "random=0\n",
        "resize=1.5\n",
        "nms_kind=greedynms\n",
        "beta_nms=0.6\n",
        "#new_coords=1\n",
        "#scale_x_y = 2.0\n",
        "\n",
        "  \"\"\"\n",
        "  out.write(st)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2LAciMh4Cut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3dcba4d-6545-4100-96ae-c3d0559288c6"
      },
      "source": [
        "!cat /content/darknet/yolov4_custom.cfg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[net]\n",
            "# Testing\n",
            "#batch=1\n",
            "#subdivisions=1\n",
            "# Training\n",
            "batch=64\n",
            "subdivisions=16\n",
            "width=416\n",
            "height=416\n",
            "channels=3\n",
            "momentum=0.9\n",
            "decay=0.0005\n",
            "angle=0\n",
            "saturation = 1.5\n",
            "exposure = 1.5\n",
            "hue=.1\n",
            "\n",
            "learning_rate=0.00261\n",
            "burn_in=1000\n",
            "\n",
            "max_batches = 104000\n",
            "policy=steps\n",
            "steps=83200,93600\n",
            "scales=.1,.1\n",
            "\n",
            "\n",
            "#weights_reject_freq=1001\n",
            "#ema_alpha=0.9998\n",
            "#equidistant_point=1000\n",
            "#num_sigmas_reject_badlabels=3\n",
            "#badlabels_rejection_percentage=0.2\n",
            "\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=32\n",
            "size=3\n",
            "stride=2\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=64\n",
            "size=3\n",
            "stride=2\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=64\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[route]\n",
            "layers=-1\n",
            "groups=2\n",
            "group_id=1\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=32\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=32\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[route]\n",
            "layers = -1,-2\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=64\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[route]\n",
            "layers = -6,-1\n",
            "\n",
            "[maxpool]\n",
            "size=2\n",
            "stride=2\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[route]\n",
            "layers=-1\n",
            "groups=2\n",
            "group_id=1\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=64\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=64\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[route]\n",
            "layers = -1,-2\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[route]\n",
            "layers = -6,-1\n",
            "\n",
            "[maxpool]\n",
            "size=2\n",
            "stride=2\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[route]\n",
            "layers=-1\n",
            "groups=2\n",
            "group_id=1\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[route]\n",
            "layers = -1,-2\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[route]\n",
            "layers = -6,-1\n",
            "\n",
            "[maxpool]\n",
            "size=2\n",
            "stride=2\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "##################################\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "filters=171\n",
            "activation=linear\n",
            "\n",
            "\n",
            "\n",
            "[yolo]\n",
            "mask = 3,4,5\n",
            "anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319\n",
            "classes=52\n",
            "num=6\n",
            "jitter=.3\n",
            "scale_x_y = 1.05\n",
            "cls_normalizer=1.0\n",
            "iou_normalizer=0.07\n",
            "iou_loss=ciou\n",
            "ignore_thresh = .7\n",
            "truth_thresh = 1\n",
            "random=0\n",
            "resize=1.5\n",
            "nms_kind=greedynms\n",
            "beta_nms=0.6\n",
            "#new_coords=1\n",
            "#scale_x_y = 2.0\n",
            "\n",
            "[route]\n",
            "layers = -4\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[upsample]\n",
            "stride=2\n",
            "\n",
            "[route]\n",
            "layers = -1, 23\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "filters=171\n",
            "activation=linear\n",
            "\n",
            "[yolo]\n",
            "mask = 1,2,3\n",
            "anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319\n",
            "classes=52\n",
            "num=6\n",
            "jitter=.3\n",
            "scale_x_y = 1.05\n",
            "cls_normalizer=1.0\n",
            "iou_normalizer=0.07\n",
            "iou_loss=ciou\n",
            "ignore_thresh = .7\n",
            "truth_thresh = 1\n",
            "random=0\n",
            "resize=1.5\n",
            "nms_kind=greedynms\n",
            "beta_nms=0.6\n",
            "#new_coords=1\n",
            "#scale_x_y = 2.0\n",
            "\n",
            "  "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWrG9EGamSpH"
      },
      "source": [
        "# Train Custom YOLOv4 Detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6miYFbvExqMd"
      },
      "source": [
        "!./darknet detector train /content/darknet/dataset/obj.data /content/darknet/yolov4_custom.cfg /content/darknet/yolov4-tiny.weights -dont_show -map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBnwpBV5ZXxQ"
      },
      "source": [
        "# Infer Custom Objects with Saved YOLOv4 Weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "id": "5JFirYP4t6BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading config, weights and net itself."
      ],
      "metadata": {
        "id": "w3UW2mMu-PXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIDENCE = 0.5\n",
        "SCORE_THRESHOLD = 0.5\n",
        "IOU_THRESHOLD = 0.5\n",
        "\n",
        "config_path = \"cfg/yolov4_custom.cfg\"\n",
        "\n",
        "weights_path = \"weights/yolov4_custom_best.weights\"\n",
        "\n",
        "labels = open(\"data/obj.names\").read().strip().split(\"\\n\")\n",
        "\n",
        "colors = np.random.randint(0, 255, size=(len(labels), 3), dtype=\"uint8\")"
      ],
      "metadata": {
        "id": "ABgoW0NbuLcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)"
      ],
      "metadata": {
        "id": "YNOjw3lquNBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading picture I'm going to test net on."
      ],
      "metadata": {
        "id": "j7RUx1OC-S-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_name = \"/content/gdrive/MyDrive/images/test_1.jpg\"\n",
        "image = cv2.imread(path_name)\n",
        "file_name = os.path.basename(path_name)\n",
        "filename, ext = file_name.split(\".\")"
      ],
      "metadata": {
        "id": "54ThqaWkuOF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h, w = image.shape[:2]\n",
        "\n",
        "blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)"
      ],
      "metadata": {
        "id": "bsiTdnVpuPhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.setInput(blob)\n",
        "\n",
        "ln = net.getLayerNames()\n",
        "ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "start = time.perf_counter()\n",
        "layer_outputs = net.forward(ln)\n",
        "time_took = time.perf_counter() - start\n",
        "print(f\"Потребовалось: {time_took:.2f}s\")"
      ],
      "metadata": {
        "id": "CCusr5JyuRZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting predictions."
      ],
      "metadata": {
        "id": "0UhlAQl4-Xlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "font_scale = 1\n",
        "thickness = 1\n",
        "boxes, confidences, class_ids = [], [], []\n",
        "\n",
        "for output in layer_outputs:\n",
        "    \n",
        "    for detection in output:\n",
        "        \n",
        "        scores = detection[5:]\n",
        "        class_id = np.argmax(scores)\n",
        "        confidence = scores[class_id]\n",
        "        \n",
        "        if confidence > CONFIDENCE:\n",
        "            \n",
        "            box = detection[:4] * np.array([w, h, w, h])\n",
        "            (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "           \n",
        "            x = int(centerX - (width / 2))\n",
        "            y = int(centerY - (height / 2))\n",
        "           \n",
        "            boxes.append([x, y, int(width), int(height)])\n",
        "            confidences.append(float(confidence))\n",
        "            class_ids.append(class_id)"
      ],
      "metadata": {
        "id": "hQwatSKfuShb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = cv2.dnn.NMSBoxes(boxes, confidences, SCORE_THRESHOLD, IOU_THRESHOLD)"
      ],
      "metadata": {
        "id": "ZwDGhAqhuWu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(idxs) > 0:\n",
        "    \n",
        "    for i in idxs.flatten():\n",
        "        \n",
        "        x, y = boxes[i][0], boxes[i][1]\n",
        "        w, h = boxes[i][2], boxes[i][3]\n",
        "        \n",
        "        color = [int(c) for c in colors[class_ids[i]]]\n",
        "        cv2.rectangle(image, (x, y), (x + w, y + h), color=color, thickness=thickness)\n",
        "        text = f\"{labels[class_ids[i]]}: {confidences[i]:.2f}\"\n",
        "        \n",
        "        (text_width, text_height) = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, fontScale=font_scale, thickness=thickness)[0]\n",
        "        text_offset_x = x\n",
        "        text_offset_y = y - 5\n",
        "        box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height))\n",
        "        overlay = image.copy()\n",
        "        cv2.rectangle(overlay, box_coords[0], box_coords[1], color=color, thickness=cv2.FILLED)\n",
        "        \n",
        "        image = cv2.addWeighted(overlay, 0.6, image, 0.4, 0)\n",
        "     \n",
        "        cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            fontScale=font_scale, color=(0, 0, 0), thickness=thickness)"
      ],
      "metadata": {
        "id": "K2YzDPhSuYPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the image."
      ],
      "metadata": {
        "id": "uEJE8cE1-aF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imwrite(filename + \"_yolo4-tiny-74k.\" + ext, image)"
      ],
      "metadata": {
        "id": "QaF5IRIkuYls"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}